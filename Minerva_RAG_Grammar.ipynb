{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTcAajSgQ28Hm4T7UZLeQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikpournastaran/Minerva-RAG-Grammar/blob/main/Minerva_RAG_Grammar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 0. INSTALL DEPENDENCIES (ÿßŸàŸÑ ŸÜÿµÿ® ÿÆŸàÿØ⁄©ÿßÿ±)\n",
        "# ==========================================\n",
        "import os\n",
        "print(\"‚è≥ Installing libraries... Please wait (approx 1 min)...\")\n",
        "os.system(\"pip install -q gradio openai-whisper transformers accelerate bitsandbytes gtts langchain langchain-community langchain-core\")\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. IMPORTS & CONFIGURATION\n",
        "# ==========================================\n",
        "import gradio as gr\n",
        "import whisper\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "# ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ langchain_core ÿ®ÿ±ÿß€å ÿ≥ÿßÿ≤⁄Øÿßÿ±€å ÿ®ÿß ŸÜÿ≥ÿÆŸá‚ÄåŸáÿß€å ÿ¨ÿØ€åÿØ\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "\n",
        "LANG = \"it\"\n",
        "WHISPER_MODEL_SIZE = \"small\"\n",
        "LLM_MODEL_ID = \"sapienzanlp/Minerva-7B-instruct-v1.0\"\n",
        "\n",
        "# Default system instruction\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
        "You are an advanced Italian Grammar Assistant.\n",
        "Your task is to analyze the user's spoken sentence based on the provided context (examples).\n",
        "- If the input is a question with options (A/B), choose the correct one.\n",
        "- If the input requires grammatical analysis (subject, verb, etc.), perform the analysis.\n",
        "- If the input is a general sentence, correct any errors.\n",
        "Keep the answer concise and strictly follow the format of the provided examples.\n",
        "\"\"\"\n",
        "\n",
        "current_knowledge_base = \"\"\n",
        "\n",
        "# ==========================================\n",
        "# 2. MODEL LOADING\n",
        "# ==========================================\n",
        "print(\"--- [1/2] Loading Whisper Model (Hearing) ---\")\n",
        "whisper_model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
        "\n",
        "print(\"--- [2/2] Loading Minerva LLM (Brain) ---\")\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0\n",
        ")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_ID,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading LLM: {e}\")\n",
        "    print(\"Ensure you are using T4 GPU runtime (Runtime > Change runtime type > T4 GPU)\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def update_context(files):\n",
        "    global current_knowledge_base\n",
        "    context_text = \"\"\n",
        "    if not files:\n",
        "        return \"No files uploaded. Using default prompt.\"\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                context_text += f\"\\n--- Content from {os.path.basename(file.name)} ---\\n{content}\\n\"\n",
        "        except Exception as e:\n",
        "            return f\"Error reading file: {e}\"\n",
        "\n",
        "    current_knowledge_base = context_text\n",
        "    return \"‚úÖ Knowledge Base Updated! AI is ready.\"\n",
        "\n",
        "def text_to_speech(text, lang=\"it\"):\n",
        "    try:\n",
        "        if not text: return None\n",
        "        tts = gTTS(text=text, lang=lang)\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as fp:\n",
        "            tts.save(fp.name)\n",
        "            return fp.name\n",
        "    except Exception as e:\n",
        "        print(f\"TTS Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==========================================\n",
        "# 4. CORE LOGIC\n",
        "# ==========================================\n",
        "\n",
        "def process_pipeline(audio_path):\n",
        "    global current_knowledge_base\n",
        "\n",
        "    if audio_path is None:\n",
        "        return \"No audio recorded.\", None\n",
        "\n",
        "    # Step 1: Transcribe\n",
        "    print(\"üé§ Listening...\")\n",
        "    transcription_result = whisper_model.transcribe(audio_path, language=\"it\")\n",
        "    user_text = transcription_result[\"text\"]\n",
        "    print(f\"üìù Transcribed: {user_text}\")\n",
        "\n",
        "    # Step 2: Prepare Context\n",
        "    final_context = DEFAULT_SYSTEM_PROMPT\n",
        "    if current_knowledge_base:\n",
        "        final_context += f\"\\n\\n### REFERENCE EXAMPLES:\\n{current_knowledge_base}\"\n",
        "\n",
        "    # Step 3: Prompt Engineering\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"Instructions:\\n{system_context}\\n\\n\"\n",
        "        \"User Input:\\n{user_text}\\n\\n\"\n",
        "        \"Assistant Response (Italian):\"\n",
        "    )\n",
        "\n",
        "    formatted_prompt = prompt_template.format(\n",
        "        system_context=final_context,\n",
        "        user_text=user_text\n",
        "    )\n",
        "\n",
        "    # Step 4: Generate\n",
        "    print(\"ü§ñ Thinking...\")\n",
        "    full_response = llm.invoke(formatted_prompt)\n",
        "    clean_response = full_response.replace(formatted_prompt, \"\").strip()\n",
        "\n",
        "    if \"Assistant Response (Italian):\" in clean_response:\n",
        "        clean_response = clean_response.split(\"Assistant Response (Italian):\")[-1].strip()\n",
        "\n",
        "    # Step 5: Speak\n",
        "    print(\"üîä Speaking...\")\n",
        "    audio_output_path = text_to_speech(clean_response)\n",
        "\n",
        "    display_text = (\n",
        "        f\"üó£Ô∏è **Transcription:** {user_text}\\n\"\n",
        "        f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\"\n",
        "        f\"ü§ñ **AI Analysis:**\\n{clean_response}\"\n",
        "    )\n",
        "\n",
        "    return display_text, audio_output_path\n",
        "\n",
        "# ==========================================\n",
        "# 5. GRADIO APP\n",
        "# ==========================================\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Thesis: Italian AI\") as demo:\n",
        "    gr.Markdown(\"# üáÆüáπ Thesis Project: Italian Grammar AI\")\n",
        "    gr.Markdown(\"1. Upload your `.txt` files (grammar rules).\\n2. Record your voice.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 1. Upload Context\")\n",
        "            file_uploader = gr.File(label=\"Upload .txt files\", file_count=\"multiple\", file_types=[\".txt\"])\n",
        "            upload_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "            upload_btn = gr.Button(\"Update Knowledge Base\", variant=\"secondary\")\n",
        "            upload_btn.click(fn=update_context, inputs=file_uploader, outputs=upload_status)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### 2. Speak\")\n",
        "            audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"Input\")\n",
        "            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
        "            text_output = gr.Markdown(label=\"Result\")\n",
        "            audio_output = gr.Audio(label=\"Audio Response\", autoplay=True)\n",
        "\n",
        "    analyze_btn.click(fn=process_pipeline, inputs=audio_input, outputs=[text_output, audio_output])\n",
        "\n",
        "print(\"üöÄ Starting app...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxoXVBbCk19D",
        "outputId": "d58786fc-ae31-4c16-bbae-60d43a370b79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9bf2b48d83f8122d39.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}